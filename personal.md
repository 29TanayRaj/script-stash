# Documentation for RAG + LLM Powered Chatbot

**Objective**:
Create a chatbot that can answer questions based on the content of a provided PDF document. If the chatbot cannot find the answer in the PDF, it should respond:
*“Sorry, I didn’t understand your question. Do you want to connect with a live agent?”*

## Solution Proposed
A Rag+LLM based chatbot that combines **Retrieval-Augmented Generation (RAG)** with Large Language Models (LLMs) to provide a context-aware conversational interface. The system enables users to interact with the chatbot and leverage contextual information from uploaded documents for accurate and dynamic responses.

### Key Features
1. **Document Upload and Processing**: Accepts PDFs to create a searchable database of chunks for retrieval-based question answering.
2. **Prebuilt Database Support**: Supports the use of prebuilt FAISS databases for quicker deployment.
3. **Customizable Chunking**: Allows fine-tuning of document chunk sizes and overlaps for efficient processing.
4. **Contextual Responses**: Retrieves relevant information from the database to answer user queries.
5. **Chat History Management**: Includes options to clear or restore chat history for session control.
6. **Interactive UI**: Built with Streamlit for a user-friendly interface.


### Tech Stack Used
1. **Ollama** - For running local large language models.
2. **Langchain** - For managing the entier workflow of the system.
3. **Fiass** - For storing the vector embeddings of the document (chunks).
4. **OllamaEmbeddings** - For generating embeddings for the vector Database.
5. **normic-embeded-text** - LLM used for generating vector embeddings from document (chunks).
6. **PyPDFLoader** - For loading the PDF file.
7. **Pickle Library**- For saving the Fiass Database, so that it can be reused.
8. **Streamlit** - For front end of the system.
9. **Programming language** - Python

Note: For most of the tech stack wrappers from the langchain_community was used (can be observed in the required libraries section)

### Required Libraries
```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.messages import HumanMessage, AIMessage
from langchain_community.llms.ollama import Ollama
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import OllamaEmbeddings
from langchain_community.vectorstores import FAISS
import numpy as np
import pickle
import streamlit as st
```

### Custom Class
The **`OllamaEmbeddingsNormalized`** class ensures embeddings are normalized for better similarity comparisons:
```python
class OllamaEmbeddingsNormalized(OllamaEmbeddings):
    def _process_emb_response(self, input: str) -> list[float]:
        emb = super()._process_emb_response(input)
        return (np.array(emb) / np.linalg.norm(emb)).tolist()
```

- Need for this custom class arose while creating of the fallback, “Sorry, I didn’t understand your question. Do you want to connect with a live agent?”.
- The Idea is to use the 'similarity_search_with_relevance_scores' method from the Fiass database to generate similarity scores between 0 and 1.
- The Problem with normal ollama embeedings was that it does not normalize the vectors, so the similarity score can come in bery high values (in magnitute), as the default calculation is done euclidian norm.
- The way to fix this was to normalize the vectors in the embeddings generated by the OllamaEmbeddings. 



## User Interface
### Sidebar Components
#### Model Selection
- Dropdown to select the LLM model: `llama3.2`, `llama3.1`, `mistral`, `gemma2`, `llava`.

#### Database Handling
1. **Upload Prebuilt Database**: Accepts `.pkl` files for quick initialization.
2. **PDF Upload**: Accepts `.pdf` files for document-based database creation.

#### Chunking Parameters
- `Chunk Size`: Slider to adjust chunk size (100–1000 characters).
- `Chunk Overlap`: Slider to adjust chunk overlap (0–200 characters).

#### Actions
- **Create Database**: Processes the uploaded PDF and generates a FAISS database.
- **Clear Chat History**: Clears the conversation history.
- **Undo Clear**: Restores the last cleared chat history.



## Core Functionalities
### Document Processing and Database Creation
1. **PDF Loading**:
   - Uses `PyPDFLoader` to load content from the uploaded PDF.
2. **Text Splitting**:
   - Employs `RecursiveCharacterTextSplitter` to divide the content into chunks.
3. **Database Initialization**:
   - Converts document chunks into embeddings and stores them in a FAISS vector database.
4. **Database Download**:
   - Provides build database in a pickle file for future use. (To save time in the embedding process)

### Chat Functionality
#### Conversation Handling
- **User Input**: Accepts questions via a chat input field.
- **AI Responses**: Fetches answers using RAG or directly from the LLM based on the database's availability. If the database if available it uses the 'Knowlege Base' Provided to it as input.

#### Chat History Display
- Uses `st.chat_message` to render user and AI messages in the conversation flow.

#### Query Execution
1. **Database-Enhanced Responses**:
   - Retrieves relevant chunks using FAISS's `similarity_search_with_relevance_scores`.
   - Filters results with a relevance score > 0.2. (The threshold is decided heuristically, futher study is need to select a appropriate threshold).
   - This allows us to introduce the fallback in the system.
2. **Prompt Preparation**:
   - Constructs a dynamic template incorporating chat history and context for optimal responses.
3. **LLM Execution**:
   - Uses `Ollama` to generate answers in a streaming format.

#### Default Responses
- If no context is retrieved:
  - Responds with: *"Sorry, I didn't understand your question. Do you want to connect to a live agent?"*


## Code Structure
### Global Variables
- **`docs`**: Holds the processed document.
- **`documents`**: Stores the chunked document data.
- **`st.session_state`**:
  - **`database`**: Stores the FAISS database.
  - **`chat_history`**: Maintains user-AI conversation history.
  - **`backup_history`**: Temporarily holds chat history for restoration.

### Helper Functions
#### `get_response`
Handles querying the database and formatting the LLM's response:
```python
def get_response(User_question, chat_history, LLM_model, db):
    if db:
        # Retrieve relevant chunks
        search_docs = db.similarity_search_with_relevance_scores(User_question, k=5)
        results, scores = [], []
        for result in search_docs:
            results.append(result[0])
            scores.append(result[1])

        context = [result for result, score in zip(results, scores) if score > 0.2]

        template = """
        You are a RAG assistant...
        """
    else:
        template = """
        You are a helpful assistant...
        """
    # Prompt creation and response generation
    prompt = ChatPromptTemplate.from_template(template)
    llm = Ollama(model=LLM_model)
    Output_Parser = StrOutputParser()
    chain = prompt | llm | Output_Parser

    if db:
        return chain.stream({'chat_history': chat_history, 'User_question': User_question, 'context': context})
    else:
        return chain.stream({'chat_history': chat_history, 'User_question': User_question})
```

### Conversation Display
Loops through `st.session_state.chat_history` to render chat messages in the UI.



## Usage Instructions
1. Launch the application:
   ```bash
   streamlit run app.py
   ```
2. Configure model and upload database or PDF via the sidebar.
3. Interact with the chatbot by entering questions in the chat input.
4. Manage chat history using the provided sidebar options.



## Future Enhancements
- **Multi-PDF Support**: Enable processing multiple PDFs.
- **Advanced Filtering**: Improve context filtering with more sophisticated scoring mechanisms.
- **Integration with Live Agents**: Allow seamless handoffs to human support agents.

---

