{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytest\n",
    "\n",
    "Pytest is a testing framework for Python designed to help you write better code with less effort:\n",
    "\n",
    "*   **Simple Syntax**: Uses plain Python `assert` statements, making tests readable and easy to write.\n",
    "*   **Highly Scalable**: Works efficiently for small scripts and large-scale applications.\n",
    "*   **Rich Ecosystem**: Offers a massive library of plugins for various specialized testing needs.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Test Discovery and Naming Conventions\n",
    "\n",
    "For Pytest to automatically find and execute your tests, you must follow specific naming conventions:\n",
    "\n",
    "*   **Test Files**: Should be named `test_*.py` or `*_test.py`. For example, `test_api.py` or `logic_test.py`.\n",
    "*   **Test Functions**: Must start with the prefix `test_`. For example, `def test_addition():`.\n",
    "*   **Test Classes**: Should start with `Test` and should not have an `__init__` method. For example, `class TestCalculator:`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Command Line Usage\n",
    "\n",
    "Pytest provides a powerful CLI to control how your tests run. Here are the most common commands:\n",
    "\n",
    "### Basic Execution\n",
    "*   `pytest`: Runs all tests in the current directory and subdirectories that match the naming conventions.\n",
    "*   `pytest test_api.py`: Runs tests in a specific file.\n",
    "*   `pytest tests/`: Runs all tests within a specific folder.\n",
    "\n",
    "### Useful Flags\n",
    "*   `-v` (Verbose): Shows more detail for each test, including the test names.\n",
    "*   `-s`: Disables output capturing, allowing you to see `print()` statements in the terminal during test execution.\n",
    "*   `-x`: Stops the test execution immediately after the first failure.\n",
    "*   `-k \"expression\"`: Only runs tests whose names match the expression. For example, `pytest -k \"borrow\"` runs all tests with \"borrow\" in their name.\n",
    "*   `--lf` (Last Failed): Only runs the tests that failed during the last run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Core Pytest Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Assertions\n",
    "We use the standard Python `assert` statement to check for expected results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "def add(a, b):\n",
    "    return a + b\n",
    "\n",
    "def test_add_basic():\n",
    "    # We are testing if our add function returns 3 when given 1 and 2\n",
    "    assert add(1, 2) == 3\n",
    "\n",
    "# To run this in a notebook, you can call it directly or use a test runner\n",
    "test_add_basic()\n",
    "print(\"Test passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing for Expected Errors\n",
    "Sometimes, you need to ensure your code fails correctly when given invalid data. We use `pytest.raises` to verify that a specific exception is thrown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception test passed!\n"
     ]
    }
   ],
   "source": [
    "import pytest\n",
    "\n",
    "def test_add_error():\n",
    "    # This test passes if a TypeError is raised\n",
    "    with pytest.raises(TypeError):\n",
    "        add(1, \"invalid_input\")\n",
    "\n",
    "test_add_error()\n",
    "print(\"Exception test passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data-Driven Testing (Parametrization)\n",
    "Instead of writing multiple test functions for the same logic, you can use `@pytest.mark.parametrize` to run one test with different inputs. This is especially useful for edge cases like floating-point math."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Parametrize usually requires running via 'pytest' command line\n",
    "# In a script, it looks like this:\n",
    "\"\"\"\n",
    "@pytest.mark.parametrize(\"a, b, expected\", [\n",
    "    (10, 20, 30),\n",
    "    (0.1, 0.2, pytest.approx(0.3)),\n",
    "])\n",
    "def test_add_parameterized(a, b, expected):\n",
    "    assert add(a, b) == expected\n",
    "\"\"\"\n",
    "print(\"Check demo_basic.py for full parametrization example.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Fixtures\n",
    "Fixtures are functions that run before (Setup) and after (Teardown) your tests. They are perfect for preparing databases or cleaning up temporary files. Using `yield` allows you to define the cleanup logic within the same function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pytest.fixture\n",
    "def temp_db():\n",
    "    # Setup: prepare the resource\n",
    "    db = {\"users\": [\"Alice\", \"Bob\"]}\n",
    "    yield db \n",
    "    # Teardown: clean up after the test finishes\n",
    "    db.clear()\n",
    "\n",
    "print(\"Fixtures are typically used in test files (e.g., demo_basic.py).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. The Backend Server (main.py)\n",
    "\n",
    "We have implemented a Library Management System using FastAPI and SQLAlchemy. This server demonstrates how to handle data persistence using an in-memory SQLite database.\n",
    "\n",
    "### Running the Server\n",
    "To start the server with the auto-reload feature enabled, run the following command in your terminal:\n",
    "```bash\n",
    "python run.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. API Testing (test_api.py)\n",
    "\n",
    "Once the server is running, we need to verify that our endpoints work correctly. We use the FastAPI `TestClient` to simulate HTTP requests without needing to open a browser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example API Test\n",
    "```python\n",
    "from fastapi.testclient import TestClient\n",
    "from main import app\n",
    "\n",
    "client = TestClient(app)\n",
    "\n",
    "def test_borrow_and_return():\n",
    "    # Step 1: Borrow the book\n",
    "    borrow_res = client.post(\"/books/1/borrow\")\n",
    "    assert borrow_res.status_code == 200\n",
    "    assert borrow_res.json()[\"is_borrowed\"] is True\n",
    "\n",
    "    # Step 2: Return the book\n",
    "    return_res = client.post(\"/books/1/return\")\n",
    "    assert return_res.status_code == 200\n",
    "    assert return_res.json()[\"is_borrowed\"] is False\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. How to Execute the Demo\n",
    "\n",
    "Run these commands in your terminal to execute the tests:\n",
    "\n",
    "```bash\n",
    "pytest -v\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
